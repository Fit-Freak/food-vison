{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "We are trying to impement an AI Object Detection System that can\n",
        "1. Identify multiple food items on a plate under different lighting conditions.\n",
        "2. Create segmentation masks.\n",
        "3. Predict the food item and estimate portion size."
      ],
      "metadata": {
        "id": "LGnP4eT26hWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setting Up Platform\n",
        "\n",
        "Setting up the platform with updated libraries and device agonistic code."
      ],
      "metadata": {
        "id": "lAzyzK1J8oXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-KSNNSJ_6JeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6be231-4cbc-4b12-bc91-281127217df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ torch version: 2.8.0+cu126\n",
            "✅ torchvision version: 0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# --- Proper version checking ---\n",
        "from packaging import version\n",
        "import torchvision\n",
        "\n",
        "required_torch = \"1.12.0\"\n",
        "required_torchvision = \"0.13.0\"\n",
        "\n",
        "if version.parse(torch.__version__) < version.parse(required_torch) or \\\n",
        "   version.parse(torchvision.__version__) < version.parse(required_torchvision):\n",
        "    print(\"[INFO] torch/torchvision versions not as required, installing latest versions.\")\n",
        "    # You can change cu121 to cu118 or cpu as needed\n",
        "    os.system(\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
        "    # Reload torch and torchvision to reflect new versions\n",
        "    import importlib\n",
        "    importlib.reload(torch)\n",
        "    importlib.reload(torchvision)\n",
        "\n",
        "print(f\"✅ torch version: {torch.__version__}\")\n",
        "print(f\"✅ torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's follow best practice and setup device-agnostic code.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via Runtime -> Change runtime type -> Hardware accelerator -> GPU. **If you do this, your runtime will likely reset and you'll have to run all of the cells above by going Runtime -> Run before.**\n",
        "\n",
        "If you are running on a CPU-only machine, please use `torch.load()` with `map_location=torch.device('cpu')` to map your storages to the **CPU**."
      ],
      "metadata": {
        "id": "_jyU7Uw388Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t6vsB5IR9LQN",
        "outputId": "a4c4c10b-b0e3-4660-dfd5-94382827b637"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load PreTrained Model"
      ],
      "metadata": {
        "id": "U1TaFlSA9q6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Setup Model directory\n",
        "MODEL_DIR = Path(\"models\")\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "6ahEuhXj9SXl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload Pretrainned Model\n",
        "if MODEL_DIR.is_dir():\n",
        "  print(f\"✅ Model directory exists: {MODEL_DIR.resolve()}\")\n",
        "else:\n",
        "  print(f\"Model directory does not exist, creating: {MODEL_DIR.resolve()}\")\n",
        "  MODEL_DIR.mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJe10Pp-QEI",
        "outputId": "668b9240-eaf8-48e1-e178-3fb33abb04e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model directory exists: /content/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load The model\n",
        "vit_base_224_relem = torch.load(MODEL_DIR/\"VIT_base_224_ReLeM.pth\")\n",
        "vit_base_224_relem.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "UYz6ypm0-_5D",
        "outputId": "a0d3af78-b5d9-44e6-c581-259a4ec28385"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3130391596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load The model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvit_base_224_relem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"VIT_base_224_ReLeM.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvit_base_224_relem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                     warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the Model"
      ],
      "metadata": {
        "id": "mEynyWsPCcTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model size in bytes and convert to megabytes\n",
        "pretrainded_relem_vit_b_16_model_size = MODEL_PATH.stat().st_size / (1024 * 1024)\n",
        "print(f\"Pretrained EffNetB2 Feature Extractor model size: {round(pretrainded_relem_vit_b_16_model_size, 2)} MB\")"
      ],
      "metadata": {
        "id": "zgQmJu9s9pkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}