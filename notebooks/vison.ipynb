{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "076e0c79",
      "metadata": {
        "id": "076e0c79"
      },
      "source": [
        "# Food Vision With SegFormer and Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "Ad_gK2nNk7tO"
      },
      "id": "Ad_gK2nNk7tO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Helpers"
      ],
      "metadata": {
        "id": "a_zEzsemL651"
      },
      "id": "a_zEzsemL651"
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_image_with_mask(image, mask=None, alpha=0.5, ignore_index=255):\n",
        "    \"\"\"\n",
        "    Visualize an image, its segmentation mask, and overlay.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray or PIL.Image.Image): RGB image.\n",
        "        mask (np.ndarray or PIL.Image.Image): Segmentation mask (H x W).\n",
        "        alpha (float): Transparency for overlay blending.\n",
        "        ignore_index (int): Mask value to ignore (e.g., 255).\n",
        "    \"\"\"\n",
        "    # --- Convert image to NumPy ---\n",
        "    img = np.array(image)\n",
        "    if img.ndim == 2:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    if img.dtype != np.uint8:\n",
        "        img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    mask_np = None\n",
        "    if mask is not None:\n",
        "        mask_np = np.array(mask, dtype=np.int64)\n",
        "\n",
        "        # --- Resize mask to match image if needed ---\n",
        "        if mask_np.shape[:2] != (h, w):\n",
        "            mask_np = cv2.resize(mask_np, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # --- Deterministic color map (tab20 colormap) ---\n",
        "        cmap = plt.get_cmap(\"tab20\", np.max(mask_np) + 1)\n",
        "        color_mask = np.zeros_like(img, dtype=np.uint8)\n",
        "\n",
        "        for cls_id in np.unique(mask_np):\n",
        "            if cls_id == ignore_index:\n",
        "                color = (128, 128, 128)  # gray for ignore\n",
        "            elif cls_id == 0:\n",
        "                color = (30, 30, 30)  # dark gray for background\n",
        "            else:\n",
        "                color = (np.array(cmap(cls_id)[:3]) * 255).astype(np.uint8)\n",
        "            color_mask[mask_np == cls_id] = color\n",
        "\n",
        "        # --- Blend overlay ---\n",
        "        blended = cv2.addWeighted(img, 1 - alpha, color_mask, alpha, 0)\n",
        "    else:\n",
        "        blended = img\n",
        "\n",
        "    # --- Plot ---\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"ðŸ–¼ï¸ Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if mask_np is not None:\n",
        "        plt.imshow(mask_np, cmap=\"tab20\", vmin=0, vmax=np.max(mask_np))\n",
        "        plt.title(\"ðŸŽ­ Segmentation Mask\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No mask\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(blended)\n",
        "    plt.title(\"âœ¨ Overlay (Image + Mask)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Print metadata ---\n",
        "    if mask_np is not None:\n",
        "        valid = mask_np != ignore_index\n",
        "        coverage = np.count_nonzero(valid) / mask_np.size * 100\n",
        "        print(f\"âœ… Mask coverage: {coverage:.2f}% of image area\")\n",
        "        print(f\"ðŸŸ¢ Unique classes: {np.unique(mask_np[valid]).tolist()}\")\n",
        "        print(f\"ðŸ“ Image size: {img.shape}\")\n",
        "        print(f\"ðŸŽ­ Mask size: {mask_np.shape}\")"
      ],
      "metadata": {
        "id": "GoK_6_1D6WDz"
      },
      "id": "GoK_6_1D6WDz",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def unnormalize_image(\n",
        "    tensor: torch.Tensor,\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225)\n",
        "):\n",
        "    \"\"\"\n",
        "    Reverse ImageNet normalization to make images display correctly.\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Normalized image tensor (C, H, W)\n",
        "    Returns:\n",
        "        torch.Tensor: Unnormalized tensor in [0, 1]\n",
        "    \"\"\"\n",
        "    tensor = tensor.detach().cpu()\n",
        "    if tensor.ndim != 3 or tensor.shape[0] != 3:\n",
        "        raise ValueError(f\"Expected shape (3,H,W), got {tensor.shape}\")\n",
        "    mean = torch.tensor(mean).view(3, 1, 1)\n",
        "    std = torch.tensor(std).view(3, 1, 1)\n",
        "    tensor = tensor * std + mean\n",
        "    return torch.clamp(tensor, 0, 1)"
      ],
      "metadata": {
        "id": "jfdmE5kML5xY"
      },
      "id": "jfdmE5kML5xY",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_segmentation(images, masks, preds, n=4, save_path=None, unnormalize=True):\n",
        "    \"\"\"\n",
        "    Visualize input images, ground truth masks, and model predictions.\n",
        "    Args:\n",
        "        images: (B,C,H,W)\n",
        "        masks:  (B,H,W)\n",
        "        preds:  (B,H,W)\n",
        "    \"\"\"\n",
        "    images, masks, preds = images.cpu(), masks.cpu(), preds.cpu()\n",
        "    plt.figure(figsize=(12, n * 3))\n",
        "    cmap = plt.get_cmap(\"tab20\")\n",
        "\n",
        "    for i in range(min(n, images.size(0))):\n",
        "        img = unnormalize_image(images[i]) if unnormalize else torch.clamp(images[i], 0, 1)\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # --- Original ---\n",
        "        plt.subplot(n, 3, i * 3 + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # --- Ground Truth ---\n",
        "        plt.subplot(n, 3, i * 3 + 2)\n",
        "        plt.imshow(masks[i], cmap=cmap, vmin=0, vmax=torch.max(masks))\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # --- Prediction ---\n",
        "        plt.subplot(n, 3, i * 3 + 3)\n",
        "        plt.imshow(preds[i], cmap=cmap, vmin=0, vmax=torch.max(preds))\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
        "        print(f\"ðŸ–¼ï¸ Saved segmentation visualization to: {save_path}\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Lz7YhCM_j39C"
      },
      "id": "Lz7YhCM_j39C",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(results, save_path=None):\n",
        "    \"\"\"Visualize training metrics (loss, acc, mIoU) over epochs.\"\"\"\n",
        "    epochs = range(1, len(results[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 5))\n",
        "\n",
        "    # ---- Loss ----\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, results[\"train_loss\"], \"b-\", marker=\"o\", label=\"Train\")\n",
        "    plt.plot(epochs, results[\"test_loss\"], \"r-\", marker=\"o\", label=\"Val\")\n",
        "    plt.title(\"Loss per Epoch\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    # ---- Accuracy ----\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, results[\"train_acc\"], \"b-\", marker=\"o\", label=\"Train\")\n",
        "    plt.plot(epochs, results[\"test_acc\"], \"r-\", marker=\"o\", label=\"Val\")\n",
        "    plt.title(\"Pixel Accuracy per Epoch\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    # ---- mIoU ----\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, results[\"train_iou\"], \"b-\", marker=\"o\", label=\"Train\")\n",
        "    plt.plot(epochs, results[\"test_iou\"], \"r-\", marker=\"o\", label=\"Val\")\n",
        "    plt.title(\"Mean IoU per Epoch\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"mIoU\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
        "        print(f\"âœ… Saved training curves to {save_path}\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "asyVXTl1j7BP"
      },
      "id": "asyVXTl1j7BP",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_random_sample(dataset: torch.utils.data.Dataset, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Displays a random image and its mask from a PyTorch dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): A PyTorch dataset that returns (image, mask) pairs.\n",
        "    \"\"\"\n",
        "    # Pick random index\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    image, mask = dataset[idx]\n",
        "\n",
        "    # If tensors â†’ convert to NumPy (for plotting)\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image_np = image.permute(1, 2, 0).numpy()  # CHW â†’ HWC\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "    else:\n",
        "        image_np = np.array(image)\n",
        "\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask_np = mask.numpy()\n",
        "    else:\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "    # --- Print metadata ---\n",
        "    print(f\"ðŸ–¼ï¸ Sample Index: {idx}\")\n",
        "    # --- Visualize using helper ---\n",
        "    visualize_image_with_mask(image_np, mask_np)"
      ],
      "metadata": {
        "id": "VeH8PRas6hus"
      },
      "id": "VeH8PRas6hus",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric Helpers"
      ],
      "metadata": {
        "id": "UDsOUbADMHgR"
      },
      "id": "UDsOUbADMHgR"
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_accuracy(preds, labels, ignore_index: int = 255):\n",
        "    \"\"\"Compute per-pixel accuracy (ignoring 'ignore_index' pixels).\"\"\"\n",
        "    valid_mask = labels != ignore_index\n",
        "    correct = (preds[valid_mask] == labels[valid_mask]).sum().item()\n",
        "    total = valid_mask.sum().item()\n",
        "    return correct / total if total > 0 else 0.0"
      ],
      "metadata": {
        "id": "b59CuLoWj_lO"
      },
      "id": "b59CuLoWj_lO",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(preds, labels, num_classes: int, ignore_index: int = 255):\n",
        "    \"\"\"Compute mean Intersection-over-Union (mIoU), ignoring 'ignore_index' pixels.\"\"\"\n",
        "    preds = preds.detach().cpu()\n",
        "    labels = labels.detach().cpu()\n",
        "\n",
        "    # Mask out ignored pixels\n",
        "    mask = labels != ignore_index\n",
        "    preds = preds[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        label_inds = labels == cls\n",
        "        intersection = (pred_inds & label_inds).sum().item()\n",
        "        union = (pred_inds | label_inds).sum().item()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "\n",
        "    return np.mean(ious) if ious else 0.0"
      ],
      "metadata": {
        "id": "3xXwxXteMKhA"
      },
      "id": "3xXwxXteMKhA",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint Utilities"
      ],
      "metadata": {
        "id": "CGszaJPHMQ6L"
      },
      "id": "CGszaJPHMQ6L"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer=None, scheduler=None, epoch=0, best_miou=0.0, path=\"ckpts\", filename=None):\n",
        "    \"\"\"\n",
        "    Save model weights + optimizer + scheduler state (for training resume).\n",
        "    \"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    filename = filename or f\"segformer_finetuned_epoch_{epoch}.ckpt\"\n",
        "    checkpoint_path = os.path.join(path, filename)\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict() if optimizer else None,\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
        "        \"best_miou\": best_miou,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"âœ… Checkpoint saved: {checkpoint_path} (Best mIoU: {best_miou:.4f})\")\n",
        "\n",
        "def save_huggingface_export(model, processor, export_dir=\"./ckpts/segformer_finetuned\"):\n",
        "    \"\"\"\n",
        "    Save model + processor in Hugging Face format (for inference and sharing).\n",
        "    \"\"\"\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "    model.save_pretrained(export_dir)\n",
        "    processor.save_pretrained(export_dir)\n",
        "    print(f\"ðŸŽ¯ Exported Hugging Face model and processor to: {export_dir}\")"
      ],
      "metadata": {
        "id": "4BIoNMNOMLaS"
      },
      "id": "4BIoNMNOMLaS",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "\n",
        "def load_checkpoint(model, path, weights_only: bool = False, optimizer=None, scheduler=None, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load checkpoint (for resuming training).\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device, weights_only=weights_only)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    if optimizer and checkpoint.get(\"optimizer_state_dict\"):\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    if scheduler and checkpoint.get(\"scheduler_state_dict\"):\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "    epoch = checkpoint.get(\"epoch\", 0)\n",
        "    best_miou = checkpoint.get(\"best_miou\", 0.0)\n",
        "    print(f\"âœ… Loaded checkpoint from epoch {epoch+1} | Best mIoU={best_miou:.4f}\")\n",
        "    return checkpoint\n",
        "\n",
        "def load_huggingface_export(model_dir=\"./ckpts/segformer_finetuned\", device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load model and processor saved via Hugging Face API (for inference).\n",
        "    \"\"\"\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(model_dir)\n",
        "    processor = SegformerImageProcessor.from_pretrained(model_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"ðŸš€ Loaded Hugging Face model and processor from: {model_dir}\")\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "fEhMuxgVcXKA"
      },
      "id": "fEhMuxgVcXKA",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38fc581f",
      "metadata": {
        "id": "38fc581f"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "98cf102d",
      "metadata": {
        "id": "98cf102d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Tuple, Optional, Dict, List\n",
        "from enum import Enum\n",
        "import json\n",
        "# Essentials\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "# Hugging Face\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db7ae64",
      "metadata": {
        "id": "5db7ae64"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "24555e94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24555e94",
        "outputId": "553396ed-c977-4f58-b62c-f1a7cee08030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… torch version: 2.8.0+cu126\n",
            "âœ… torchvision version: 0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from packaging import version\n",
        "\n",
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "required_torch = \"1.12.0\"\n",
        "required_torchvision = \"0.13.0\"\n",
        "\n",
        "if version.parse(torch.__version__) < version.parse(required_torch) or \\\n",
        "   version.parse(torchvision.__version__) < version.parse(required_torchvision):\n",
        "    print(\"[INFO] torch/torchvision versions not as required, installing latest versions.\")\n",
        "    # You can change cu121 to cu118 or cpu as needed\n",
        "    import os\n",
        "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "    # Reload torch and torchvision to reflect new versions\n",
        "    import importlib\n",
        "    importlib.reload(torch)\n",
        "    importlib.reload(torchvision)\n",
        "\n",
        "print(f\"âœ… torch version: {torch.__version__}\")\n",
        "print(f\"âœ… torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ebc846",
      "metadata": {
        "id": "00ebc846"
      },
      "source": [
        "## Setup Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive\n",
        "\n",
        "Uncomment the lines to mount google drive if using **google colab**."
      ],
      "metadata": {
        "id": "Coj0GjGfRVPg"
      },
      "id": "Coj0GjGfRVPg"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGk5o_qbRS-I",
        "outputId": "4f5d168b-c9e2-4a97-e7e7-697e62cdc7c4"
      },
      "id": "xGk5o_qbRS-I",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1c4f17",
      "metadata": {
        "id": "1b1c4f17"
      },
      "source": [
        "## Setup Device-Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "9f2e755d",
      "metadata": {
        "id": "9f2e755d"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "e9262e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9262e45",
        "outputId": "85f62587-046b-46fc-c7fc-b0a857e73914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "print(f\"âœ… Using Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ef162f",
      "metadata": {
        "id": "61ef162f"
      },
      "source": [
        "## Setup Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "0ea96d20",
      "metadata": {
        "id": "0ea96d20"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed: int = 42):\n",
        "    \"\"\"Sets seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8e9d5be7",
      "metadata": {
        "id": "8e9d5be7"
      },
      "outputs": [],
      "source": [
        "set_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a4ce2f",
      "metadata": {
        "id": "90a4ce2f"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc112c0e",
      "metadata": {
        "id": "dc112c0e"
      },
      "source": [
        "## Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "00f8a05a",
      "metadata": {
        "id": "00f8a05a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Optional, Tuple, Literal\n",
        "from enum import Enum\n",
        "from transformers import BaseImageProcessor\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "class FoodSeg103Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Hugging Face + torchvision hybrid dataset wrapper for FoodSeg103.\n",
        "    Supports:\n",
        "        - HF processor (SegFormer-style)\n",
        "        - torchvision transforms (augmentations)\n",
        "        - both together, with configurable order\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hf_dataset,\n",
        "        processor = None,\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "        transform_order: Literal[\"before\", \"after\"] = \"before\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hf_dataset: Hugging Face dataset split (e.g., load_dataset(\"foodseg103\", split=\"train\"))\n",
        "            processor: Hugging Face image processor (handles resizing, normalization, etc.)\n",
        "            transform: torchvision transforms (for augmentation)\n",
        "            transform_order: \"before\" or \"after\" processor application\n",
        "        \"\"\"\n",
        "        self.dataset = hf_dataset\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "        self.transform_order = transform_order\n",
        "\n",
        "        if self.processor is None and self.transform is None:\n",
        "            print(\"âš ï¸ Warning: No processor or transform provided.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = self.dataset[idx]\n",
        "      image = item[\"image\"]   # PIL Image\n",
        "      label = item[\"label\"]   # PIL mask\n",
        "\n",
        "      # ======================================================\n",
        "      # CASE 1 â€” Processor only\n",
        "      # ======================================================\n",
        "      if self.processor is not None and self.transform is None:\n",
        "          encoded = self.processor(image, segmentation_maps=label, return_tensors=\"pt\")\n",
        "          return encoded[\"pixel_values\"].squeeze(0), encoded[\"labels\"].squeeze(0).long()\n",
        "\n",
        "      # ======================================================\n",
        "      # CASE 2 â€” Transform only\n",
        "      # ======================================================\n",
        "      if self.processor is None and self.transform is not None:\n",
        "          image = self.transform(image)\n",
        "          if isinstance(image, Image.Image):  # if transform didnâ€™t convert to tensor\n",
        "              image = transforms.ToTensor()(image)\n",
        "          label = torch.as_tensor(np.array(label, dtype=np.int64), dtype=torch.long)\n",
        "          return image, label\n",
        "\n",
        "      # ======================================================\n",
        "      # CASE 3 â€” Processor + Transform hybrid\n",
        "      # ======================================================\n",
        "      if self.processor is not None and self.transform is not None:\n",
        "          if self.transform_order == \"before\":\n",
        "              # Apply torchvision transforms first, then processor\n",
        "              image = self.transform(image)\n",
        "              if isinstance(image, torch.Tensor):\n",
        "                  image = transforms.ToPILImage()(image)\n",
        "              encoded = self.processor(image, segmentation_maps=label, return_tensors=\"pt\")\n",
        "              pixel_values = encoded[\"pixel_values\"].squeeze(0)\n",
        "              labels = encoded[\"labels\"].squeeze(0).long()\n",
        "              return pixel_values, labels\n",
        "          else:\n",
        "              # Apply processor first, then torchvision transforms\n",
        "              encoded = self.processor(image, segmentation_maps=label, return_tensors=\"pt\")\n",
        "              image = encoded[\"pixel_values\"].squeeze(0)\n",
        "              labels = encoded[\"labels\"].squeeze(0).long()\n",
        "\n",
        "              # Convert back to PIL for torchvision transforms\n",
        "              image = transforms.ToPILImage()(image)\n",
        "              image = self.transform(image)\n",
        "              # Ensure tensor output\n",
        "              if isinstance(image, Image.Image):\n",
        "                  image = transforms.ToTensor()(image)\n",
        "              return image, labels\n",
        "\n",
        "      # ======================================================\n",
        "      # CASE Fallback\n",
        "      # ======================================================\n",
        "      image = transforms.ToTensor()(image)\n",
        "      label = torch.as_tensor(np.array(label, dtype=np.int64), dtype=torch.long)\n",
        "      return image, label\n",
        "\n",
        "    def __repr__(self):\n",
        "        transform_str = str(self.transform) if self.transform else \"None\"\n",
        "        processor_str = self.processor.__class__.__name__ if self.processor else \"None\"\n",
        "        split_name = getattr(self.dataset, \"split\", \"unknown\")\n",
        "        return (\n",
        "            f\"Dataset: FoodSeg103\\n\"\n",
        "            f\"  Split: {split_name}\\n\"\n",
        "            f\"  Samples: {len(self)}\\n\"\n",
        "            f\"  Processor: {processor_str}\\n\"\n",
        "            f\"  Transform: {transform_str}\\n\"\n",
        "            f\"  Order: {self.transform_order}\\n\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Mappings"
      ],
      "metadata": {
        "id": "_ruQ_i7wOTeA"
      },
      "id": "_ruQ_i7wOTeA"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_class_mappings(json_path: str) -> Tuple[Optional[Dict[int, str]], Optional[Dict[int, str]]]:\n",
        "    \"\"\"\n",
        "    Load id2label and label2id mappings from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        json_path (str): Path to the JSON file like:\n",
        "            {\n",
        "              \"0\": \"background\",\n",
        "              \"1\": \"candy\",\n",
        "              \"2\": \"egg tart\",\n",
        "              ...\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        tuple: (id2label: dict[int, str], label2id: dict[str, int])\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert string keys to integers for id2label\n",
        "    id2label = {int(k): v for k, v in data.items()}\n",
        "    label2id = {v: int(k) for k, v in id2label.items()}\n",
        "\n",
        "    return id2label, label2id"
      ],
      "metadata": {
        "id": "YwBs7Ub0Uo3q"
      },
      "id": "YwBs7Ub0Uo3q",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label, label2id = load_class_mappings(\"/content/drive/MyDrive/datasets/foodSeg103/classnames.json\")\n",
        "\n",
        "print(\"First 5 id2label:\")\n",
        "print(dict(list(id2label.items())[:5]))\n",
        "\n",
        "print(\"\\nFirst 5 label2id:\")\n",
        "print(dict(list(label2id.items())[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3hKVRLikoqX",
        "outputId": "6ebd3b52-4d0b-43f3-e0fc-2e2e67f7c566"
      },
      "id": "Q3hKVRLikoqX",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 id2label:\n",
            "{0: 'background', 1: 'candy', 2: 'egg tart', 3: 'french fries', 4: 'chocolate'}\n",
            "\n",
            "First 5 label2id:\n",
            "{'background': 0, 'candy': 1, 'egg tart': 2, 'french fries': 3, 'chocolate': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581c29de",
      "metadata": {
        "id": "581c29de"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "ae275c84",
      "metadata": {
        "id": "ae275c84"
      },
      "outputs": [],
      "source": [
        "class Split(Enum):\n",
        "    TRAIN = \"train\"\n",
        "    VALIDATION = \"validation\"\n",
        "    TEST = \"test\"\n",
        "    ALL = \"all\"\n",
        "\n",
        "\n",
        "def create_dataloaders_from_hf(\n",
        "    dataset_name: str,\n",
        "    split: Split = Split.ALL,\n",
        "    processor = None,\n",
        "    train_transform: Optional[transforms.Compose] = None,\n",
        "    val_transform: Optional[transforms.Compose] = None,\n",
        "    test_transform: Optional[transforms.Compose] = None,\n",
        "    transform_order: Literal[\"before\", \"after\"] = \"before\",\n",
        "    batch_size: int = 4,\n",
        "    num_workers: int = 2,\n",
        "    device: str = None,\n",
        ") -> Tuple[\n",
        "    Optional[torch.utils.data.DataLoader],\n",
        "    Optional[torch.utils.data.DataLoader],\n",
        "    Optional[torch.utils.data.DataLoader],\n",
        "]:\n",
        "    \"\"\"Creates PyTorch DataLoaders with per-split transforms + processor hybrid support.\"\"\"\n",
        "\n",
        "    def _build_loader(split_name: str, transform:Optional[transforms.Compose] = None, is_train: bool = False):\n",
        "        ds = load_dataset(dataset_name, split=split_name)\n",
        "        dataset = FoodSeg103Dataset(\n",
        "            ds,\n",
        "            processor=processor,\n",
        "            transform=transform,\n",
        "            transform_order=transform_order,\n",
        "        )\n",
        "        _device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pin_mem = True if _device == \"cuda\" else False\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=is_train,\n",
        "            drop_last=is_train,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_mem,\n",
        "        )\n",
        "\n",
        "    train_loader = val_loader = test_loader = None\n",
        "\n",
        "    if split in (Split.TRAIN, Split.ALL):\n",
        "        try:\n",
        "          train_loader = _build_loader(\"train\", transform=train_transform, is_train=True)\n",
        "        except ValueError:\n",
        "            print(f\"âš ï¸ No 'train' split found in dataset: {dataset_name}\")\n",
        "    if split in (Split.VALIDATION, Split.ALL):\n",
        "        try:\n",
        "          val_loader = _build_loader(\"validation\", transform=val_transform)\n",
        "        except ValueError:\n",
        "            print(f\"âš ï¸ No 'validation' split found in dataset: {dataset_name}\")\n",
        "    if split in (Split.TEST, Split.ALL):\n",
        "        try:\n",
        "            test_loader = _build_loader(\"test\", transform=test_transform)\n",
        "        except ValueError:\n",
        "            print(f\"âš ï¸ No 'test' split found in dataset: {dataset_name}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30aa13ae",
      "metadata": {
        "id": "30aa13ae"
      },
      "source": [
        "# Model Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7362b70f",
      "metadata": {
        "id": "7362b70f"
      },
      "source": [
        "## SegFormer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "5107111e",
      "metadata": {
        "id": "5107111e"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation, SegformerConfig, SegformerImageProcessor, AutoImageProcessor\n",
        "\n",
        "class Segformer:\n",
        "    \"\"\"\n",
        "    A flexible builder for creating and configuring SegFormer models\n",
        "    from the Hugging Face Transformers library.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_classes: int = 104,\n",
        "        ignore_mismatched_sizes: bool = True,\n",
        "        dropout: float = 0.1,\n",
        "        device: str = \"cuda\",\n",
        "        processor = None,\n",
        "        id2label: Optional[Dict[int, str]] = None,\n",
        "        label2id: Optional[Dict[str, int]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the SegformerBuilder.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Pretrained SegFormer model name or path.\n",
        "            num_classes (int): Number of segmentation classes.\n",
        "            ignore_mismatched_sizes (bool): Allow different output head sizes.\n",
        "            use_pretrained (bool): Whether to load pretrained weights.\n",
        "            device (str): Device to load model on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_mismatched_sizes = ignore_mismatched_sizes\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "\n",
        "        # Load model config Parameters\n",
        "        self.config = SegformerConfig.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_classes,\n",
        "            hidden_dropout_prob=dropout,\n",
        "            id2label=self.id2label,\n",
        "            label2id=self.label2id\n",
        "        )\n",
        "\n",
        "        # Load model weights\n",
        "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "            model_name,\n",
        "            config=self.config,\n",
        "            ignore_mismatched_sizes=ignore_mismatched_sizes\n",
        "        )\n",
        "        # Load image processor (for transforms)\n",
        "        self.processor = processor or AutoImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "        # Set model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_model(self) -> torch.nn.Module:\n",
        "        \"\"\"Return the PyTorch model.\"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def get_processor(self):\n",
        "        \"\"\"Return the Hugging Face image processor for transforms.\"\"\"\n",
        "        return self.processor\n",
        "\n",
        "    def freeze_encoder(self, freeze=True):\n",
        "        \"\"\"Freeze or unfreeze the encoder layers.\"\"\"\n",
        "        for param in self.model.segformer.encoder.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "        print(f\"Encoder frozen: {freeze}\")\n",
        "\n",
        "    def freeze_decoder(self, freeze=True):\n",
        "        \"\"\"Freeze or unfreeze the decoder layers.\"\"\"\n",
        "        for param in self.model.decode_head.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "        print(f\"Decoder frozen: {freeze}\")\n",
        "\n",
        "    def unfreeze_all(self):\n",
        "        \"\"\"Unfreeze all parameters.\"\"\"\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"All model parameters are trainable.\")\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Print basic info about the model.\"\"\"\n",
        "        print(\"\\nðŸ§© SegFormer Model Summary\\n\")\n",
        "        print(f\"Model: {self.model_name}\")\n",
        "        print(f\"Classes: {self.num_classes}\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        print(f\"Trainable: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
        "        # Add shape info\n",
        "        size = self.processor.size.get(\"height\", 512)\n",
        "        dummy = torch.randn(1, 3, size, size).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.model(pixel_values=dummy)\n",
        "        print(f\"Output keys: {list(out.keys())}\")\n",
        "        print(f\"Logits shape: {out.logits.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainning Engine"
      ],
      "metadata": {
        "id": "pZCIMM3d4A8W"
      },
      "id": "pZCIMM3d4A8W"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6ce5bf0f",
      "metadata": {
        "id": "6ce5bf0f"
      },
      "outputs": [],
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def train_step(model, dataloader, scaler, optimizer, loss_fn, num_classes: int, device: str):\n",
        "    \"\"\"Run one training epoch.\"\"\"\n",
        "    model.train()\n",
        "    train_loss, train_acc, train_iou = 0, 0, 0\n",
        "    autocast_device = \"cuda\" if device == \"cuda\" else \"cpu\"\n",
        "\n",
        "    for images, labels in tqdm(dataloader, leave=False, desc=\"Training\"):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        try:\n",
        "            # ---- Forward + Backward with AMP ----\n",
        "            with autocast(device_type=autocast_device, enabled=(device == \"cuda\")):\n",
        "                outputs = model(pixel_values=images)\n",
        "                logits = torch.nn.functional.interpolate(\n",
        "                    outputs.logits,\n",
        "                    size=labels.shape[-2:],\n",
        "                    mode=\"bilinear\",\n",
        "                    align_corners=False\n",
        "                )\n",
        "                loss = loss_fn(logits, labels)\n",
        "\n",
        "            # ---- Gradient Scaling ----\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # ---- Metrics ----\n",
        "            train_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            train_acc += pixel_accuracy(preds, labels)\n",
        "            train_iou += intersection_over_union(preds, labels, num_classes)\n",
        "\n",
        "            del loss, outputs, logits\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(\"âš ï¸ Skipping batch due to OOM error.\")\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return train_loss / n, train_acc / n, train_iou / n\n",
        "\n",
        "\n",
        "def eval_step(model, dataloader, loss_fn, num_classes: int, device: str):\n",
        "    \"\"\"Evaluate model for one epoch.\"\"\"\n",
        "    model.eval()\n",
        "    test_loss, test_acc, test_iou = 0, 0, 0\n",
        "    autocast_device = \"cuda\" if device == \"cuda\" else \"cpu\"\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in tqdm(dataloader, leave=False, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            with autocast(device_type=autocast_device, enabled=(device == \"cuda\")):\n",
        "                outputs = model(pixel_values=images)\n",
        "                logits = torch.nn.functional.interpolate(\n",
        "                    outputs.logits,\n",
        "                    size=labels.shape[-2:],\n",
        "                    mode=\"bilinear\",\n",
        "                    align_corners=False\n",
        "                )\n",
        "                loss = loss_fn(logits, labels)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_acc += pixel_accuracy(preds, labels)\n",
        "            test_iou += intersection_over_union(preds, labels, num_classes)\n",
        "\n",
        "            del loss, outputs, logits\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return test_loss / n, test_acc / n, test_iou / n\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from typing import Optional, Callable, Union, Tuple, Dict, Literal\n",
        "from pathlib import Path\n",
        "import os, time, torch, numpy as np\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Type aliases\n",
        "DataLoader = torch.utils.data.DataLoader\n",
        "Tensor = torch.Tensor\n",
        "MetricTriplet = Tuple[float, float, float]\n",
        "ResumeMode = Literal[\"auto\", \"latest\", \"best\", \"fresh\"]\n",
        "Checkpoint = Dict[str, object]\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
        "    *,\n",
        "    epochs: int,\n",
        "    num_classes: int,\n",
        "    device: Union[str, torch.device] = \"cuda\",\n",
        "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
        "    save_dir: Union[str, Path] = \"ckpts\",\n",
        "    vis_dir: Optional[Union[str, Path]] = \"visualizations\",\n",
        "    auto_save: bool = True,\n",
        "    vis_every: int = 5,\n",
        "    min_best_miou: float = 0.0,\n",
        "    max_keep_checkpoints: int = 3,\n",
        "    last_ckpt_path: Optional[Union[str, Path]] = None,\n",
        "    best_ckpt_path: Optional[Union[str, Path]] = None,\n",
        "    resume_mode: ResumeMode = \"auto\",  # \"auto\" | \"latest\" | \"best\" | \"fresh\"\n",
        "    find_latest_ckpt: Optional[Callable[[Union[str, Path]], Optional[Path]]] = None,\n",
        "    load_checkpoint: Callable[..., Checkpoint] = None,\n",
        "    save_checkpoint: Callable[..., None] = None,\n",
        "    train_step: Callable[..., MetricTriplet] = None,\n",
        "    eval_step: Callable[..., MetricTriplet] = None,\n",
        "    plot_segmentation: Optional[Callable[..., None]] = None,\n",
        "    plot_training_curves: Optional[Callable[..., None]] = None,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Tuple[Checkpoint, float, int]:\n",
        "    \"\"\"\n",
        "    Train loop for SegFormer with AMP, checkpointing, and auto-resume.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ckpt : dict\n",
        "        Last-loaded checkpoint (empty if starting fresh)\n",
        "    best_miou : float\n",
        "        Best validation mIoU achieved\n",
        "    start_epoch : int\n",
        "        Epoch to resume from\n",
        "    \"\"\"\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Setup\n",
        "    # ============================================================\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    scaler = GradScaler() if device.type == \"cuda\" else None\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Helper: Find latest checkpoint\n",
        "    # ============================================================\n",
        "    def _default_find_latest_ckpt(path: str | Path) -> Optional[Path]:\n",
        "        ckpts = [f for f in os.listdir(path) if f.startswith(\"last_checkpoint_\") and f.endswith(\".ckpt\")]\n",
        "        if not ckpts:\n",
        "            return None\n",
        "        ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "        return Path(path) / ckpts[-1]\n",
        "\n",
        "    find_latest_ckpt = find_latest_ckpt or _default_find_latest_ckpt\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Checkpoint Resume Logic\n",
        "    # ============================================================\n",
        "    latest = Path(last_ckpt_path) if last_ckpt_path else find_latest_ckpt(save_dir)\n",
        "    best = Path(best_ckpt_path) if best_ckpt_path else Path(save_dir) / \"best_segformer.ckpt\"\n",
        "\n",
        "    match resume_mode:\n",
        "        case \"latest\" if latest:\n",
        "            ckpt_path, tag = latest, \"latest\"\n",
        "        case \"best\" if best.exists():\n",
        "            ckpt_path, tag = best, \"best\"\n",
        "        case \"auto\" if latest:\n",
        "            ckpt_path, tag = latest, \"latest\"\n",
        "        case \"auto\" if best.exists():\n",
        "            ckpt_path, tag = best, \"best\"\n",
        "        case _:\n",
        "            ckpt_path, tag = None, \"fresh\"\n",
        "\n",
        "    if ckpt_path and load_checkpoint:\n",
        "        print(f\"ðŸ” Resuming from {tag} checkpoint: {ckpt_path}\")\n",
        "        ckpt = load_checkpoint(model, ckpt_path, False, optimizer, scheduler, device)\n",
        "        best_miou, start_epoch = ckpt.get(\"best_miou\", 0.0), ckpt.get(\"epoch\", 0) + 1\n",
        "    else:\n",
        "        print(\"ðŸ†• Starting fresh training...\")\n",
        "        ckpt, best_miou, start_epoch = {}, 0.0, 0\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Initialize Metrics\n",
        "    # ============================================================\n",
        "    results = {k: [] for k in [\"train_loss\", \"train_acc\", \"train_iou\", \"val_loss\", \"val_acc\", \"val_iou\", \"epoch_time\"]}\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Training Loop\n",
        "    # ============================================================\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        start_t = time.perf_counter()\n",
        "        print(f\"\\nðŸŒ± Epoch [{epoch + 1}/{epochs}]\")\n",
        "\n",
        "        train_loss, train_acc, train_iou = train_step(model, train_dataloader, scaler, optimizer, loss_fn, num_classes, device)\n",
        "        val_loss, val_acc, val_iou = eval_step(model, val_dataloader, loss_fn, num_classes, device)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_time = time.perf_counter() - start_t\n",
        "        tqdm.write(f\"ðŸ•’ {epoch_time:.2f}s | \"\n",
        "                   f\"Train: L={train_loss:.4f}, A={train_acc:.4f}, mIoU={train_iou:.4f} | \"\n",
        "                   f\"Val: L={val_loss:.4f}, A={val_acc:.4f}, mIoU={val_iou:.4f}\")\n",
        "\n",
        "        # Record metrics\n",
        "        for k, v in zip(results.keys(), [train_loss, train_acc, train_iou, val_loss, val_acc, val_iou, epoch_time]):\n",
        "            results[k].append(v)\n",
        "\n",
        "        # ========================================================\n",
        "        # ðŸ”¹ Save Checkpoints\n",
        "        # ========================================================\n",
        "        if auto_save:\n",
        "            if val_iou > best_miou and val_iou >= min_best_miou:\n",
        "                best_miou = val_iou\n",
        "                save_checkpoint(model, optimizer, scheduler, epoch, best_miou, path=save_dir, filename=\"best_segformer.ckpt\")\n",
        "                print(f\"ðŸ† New best model saved (mIoU={best_miou:.4f})\")\n",
        "\n",
        "            ckpt_file = Path(save_dir) / f\"last_checkpoint_{epoch + 1}.ckpt\"\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, best_miou, path=save_dir, filename=ckpt_file.name)\n",
        "\n",
        "            # Rotate checkpoints\n",
        "            ckpts = sorted([f for f in os.listdir(save_dir) if f.startswith(\"last_checkpoint_\")],\n",
        "                           key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "            if len(ckpts) > max_keep_checkpoints:\n",
        "                old = Path(save_dir) / ckpts[0]\n",
        "                os.remove(old)\n",
        "                print(f\"ðŸ§¹ Removed old checkpoint: {old}\")\n",
        "\n",
        "        # ========================================================\n",
        "        # ðŸ”¹ Visualization\n",
        "        # ========================================================\n",
        "        if (epoch + 1) % vis_every == 0 and plot_segmentation:\n",
        "            model.eval()\n",
        "            imgs, labels = next(iter(val_dataloader))\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            with torch.inference_mode(), autocast(device_type=device.type if device.type == \"cuda\" else \"cpu\"):\n",
        "                outputs = model(pixel_values=imgs)\n",
        "                logits = torch.nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "            plot_segmentation(imgs, labels, preds, n=min(4, len(imgs)),\n",
        "                              save_path=Path(vis_dir) / f\"vis_epoch_{epoch + 1}.png\")\n",
        "            if plot_training_curves:\n",
        "                plot_training_curves(results, save_path=Path(vis_dir) / \"training_curves.png\")\n",
        "\n",
        "    # ============================================================\n",
        "    # ðŸ”¹ Summary\n",
        "    # ============================================================\n",
        "    print(f\"\\nðŸŽ¯ Training complete! ðŸ† Best mIoU={best_miou:.4f} | â±ï¸ Avg Epoch={np.mean(results['epoch_time']):.2f}s\")\n",
        "    return ckpt, best_miou, start_epoch"
      ],
      "metadata": {
        "id": "8i2oM8v1dvPf"
      },
      "id": "8i2oM8v1dvPf",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d124c05a",
      "metadata": {
        "id": "d124c05a"
      },
      "source": [
        "## Fine Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "b909cb3f",
      "metadata": {
        "id": "b909cb3f"
      },
      "outputs": [],
      "source": [
        "# EPOCHS = 30\n",
        "# LR = 1e-4\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "# T_MAX = 20\n",
        "# IGNORE_INDEX = 255\n",
        "# CHECKPOINT_DIR = \"drive/MyDrive/ckpts\"\n",
        "# NUM_WORKERS = os.cpu_count() or 2 # Use 2 Workers as default\n",
        "# BATCH_SIZE = 4\n",
        "# NUM_CLASSES = 104\n",
        "# DROP_RATE = 0.1\n",
        "\n",
        "# # Get Class Mappings\n",
        "# id2label,label2id = load_class_mappings(\"drive/MyDrive/datasets/foodSeg103/classnames.json\")\n",
        "\n",
        "# # Initialize model and builder\n",
        "# builder = Segformer(num_classes=NUM_CLASSES, device=DEVICE, dropout=DROP_RATE, id2label=id2label, label2id=label2id)\n",
        "# model = builder.get_model()\n",
        "# # Get Image Processor from builder\n",
        "# processor = builder.get_processor()\n",
        "\n",
        "# # Initialize Dataloaders\n",
        "# train_loader, validation_loader, _ = create_dataloaders_from_hf(dataset_name=\"EduardoPacheco/FoodSeg103\", batch_size=BATCH_SIZE, processor=processor)\n",
        "\n",
        "# # Loss (ignore_index=255 if present in dataset)\n",
        "# loss_fn = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# # Optimizer and scheduler\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)\n",
        "\n",
        "# # Display Summary\n",
        "# print(builder.summary())\n",
        "\n",
        "# # Fine-tune\n",
        "# if __name__ == \"__main__\":\n",
        "#     results = train(\n",
        "#     model=model,\n",
        "#     train_dataloader=train_loader,\n",
        "#     test_dataloader=validation_loader,\n",
        "#     optimizer=optimizer,\n",
        "#     loss_fn=loss_fn,\n",
        "#     epochs=EPOCHS,\n",
        "#     device=DEVICE,\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     scheduler=scheduler,\n",
        "#     save_dir=CHECKPOINT_DIR,\n",
        "#     vis_every=10,  # visualize predictions every 10 epochs\n",
        "#     auto_save=True\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "EPOCHS = 50\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "T_MAX = EPOCHS\n",
        "IGNORE_INDEX = 255\n",
        "CHECKPOINT_DIR = \"drive/MyDrive/ckpts\"\n",
        "NUM_WORKERS = os.cpu_count() or 2 # Use 2 Workers as default\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 104\n",
        "DROP_RATE = 0.1\n",
        "\n",
        "# Get Class Mappings\n",
        "id2label,label2id = load_class_mappings(\"drive/MyDrive/datasets/foodSeg103/classnames.json\")\n",
        "\n",
        "# Initialize builder and processor\n",
        "builder = Segformer(num_classes=NUM_CLASSES, device=DEVICE, dropout=DROP_RATE, id2label=id2label, label2id=label2id)\n",
        "processor = builder.get_processor()\n",
        "print(f\"Processor: {processor.__class__.__name__}\")\n",
        "# Initialize model\n",
        "model = builder.get_model()\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”¹ Data Augmentation (Boost Generalization)\n",
        "# ============================================================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(512, scale=(0.8, 1.0)),     # scale jitter\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "])\n",
        "val_transform = None  # Keep validation clean\n",
        "\n",
        "# Initialize Dataloaders\n",
        "train_loader, validation_loader, _ = create_dataloaders_from_hf(\n",
        "    dataset_name=\"EduardoPacheco/FoodSeg103\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    processor=processor, # Applied Image Processor\n",
        "    train_transform=train_transform,\n",
        "    val_transform=val_transform,\n",
        "    transform_order=\"before\",  # Apply torchvision â†’ then processor\n",
        ")\n",
        "\n",
        "# Loss (ignore_index=255 if present in dataset)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)\n",
        "\n",
        "# Load Checkpoint\n",
        "ckpt_path = \"drive/MyDrive/ckpts/best_segformer.ckpt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
        "\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "if \"scheduler_state_dict\" in checkpoint:\n",
        "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "\n",
        "start_epoch = EPOCHS - checkpoint[\"epoch\"] + 1\n",
        "best_miou = checkpoint[\"best_miou\"]\n",
        "\n",
        "print(f\"âœ… Loaded best checkpoint from epoch {start_epoch-1} with mIoU={best_miou:.4f}\")\n",
        "\n",
        "# ðŸ”¹ Optional: lower LR to refine learning\n",
        "for g in optimizer.param_groups:\n",
        "    g[\"lr\"] = 5e-5  # reduce LR by half\n",
        "\n",
        "# ðŸ”“ Unfreeze encoder for deeper fine-tuning\n",
        "builder.unfreeze_all()\n",
        "\n",
        "print(builder.summary())\n",
        "print(\"Current LR:\", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "# Fine-tune\n",
        "if __name__ == \"__main__\":\n",
        "    results = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=validation_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=EPOCHS,\n",
        "    device=DEVICE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    scheduler=scheduler,\n",
        "    save_dir=CHECKPOINT_DIR,\n",
        "    vis_every=5,  # visualize predictions every 5 epochs\n",
        "    vis_dir=\"drive/MyDrive/ckpts/visualizations\",\n",
        "    auto_save=True,\n",
        "    train_step=train_step, # Add train_step argument\n",
        "    eval_step=eval_step, # Add eval_step argument\n",
        "    best_ckpt_path= ckpt_path,\n",
        "    resume_mode= \"best\",\n",
        "    load_checkpoint=load_checkpoint,\n",
        "    save_checkpoint=save_checkpoint,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pkTK94oaYFg",
        "outputId": "2a1b93fa-00e4-465c-ef59-45a4a9b19242"
      },
      "id": "1pkTK94oaYFg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([104]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([104, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'reduce_labels'\n",
            "  image_processor = cls(**image_processor_dict)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor: SegformerImageProcessor\n",
            "âš ï¸ No 'test' split found in dataset: EduardoPacheco/FoodSeg103\n",
            "âœ… Loaded best checkpoint from epoch 43 with mIoU=0.2656\n",
            "All model parameters are trainable.\n",
            "\n",
            "ðŸ§© SegFormer Model Summary\n",
            "\n",
            "Model: nvidia/segformer-b0-finetuned-ade-512-512\n",
            "Classes: 104\n",
            "Device: cuda\n",
            "Parameters: 3,740,872\n",
            "Trainable: 3,740,872\n",
            "Output keys: ['logits']\n",
            "Logits shape: torch.Size([1, 104, 128, 128])\n",
            "None\n",
            "Current LR: 5e-05\n",
            "ðŸ” Resuming from best checkpoint: drive/MyDrive/ckpts/best_segformer.ckpt\n",
            "âœ… Loaded checkpoint from epoch 8 | Best mIoU=0.2656\n",
            "\n",
            "ðŸŒ± Epoch [9/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ•’ 3193.31s | Train: L=1.7286, A=0.5901, mIoU=0.1245 | Val: L=1.2157, A=0.6809, mIoU=0.2426\n",
            "âœ… Checkpoint saved: drive/MyDrive/ckpts/last_checkpoint_9.ckpt (Best mIoU: 0.2656)\n",
            "\n",
            "ðŸŒ± Epoch [10/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|â–ˆâ–ˆâ–       | 266/1245 [07:30<23:44,  1.45s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854c2b4b",
      "metadata": {
        "id": "854c2b4b"
      },
      "source": [
        "## Export Checkpoint for Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e71f56",
      "metadata": {
        "id": "11e71f56"
      },
      "outputs": [],
      "source": [
        "# Save Hugging Face compatible export\n",
        "save_huggingface_export(model, processor, export_dir=\"/drive/MyDrive/models/segformer_finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "fonwwG0MuPxw"
      },
      "id": "fonwwG0MuPxw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Saved Model"
      ],
      "metadata": {
        "id": "h2nMRVBsbHQV"
      },
      "id": "h2nMRVBsbHQV"
    },
    {
      "cell_type": "code",
      "source": [
        "load_huggingface_export(model, path=\"/drive/MyDrive/models/segformer_best.ckpt\", device=builder.device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "bZmqjFThbGK9"
      },
      "id": "bZmqjFThbGK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Curves"
      ],
      "metadata": {
        "id": "QpIANSequXEw"
      },
      "id": "QpIANSequXEw"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(training_results, save_path=\"models/checkpoints/training_plots.png\")"
      ],
      "metadata": {
        "id": "jrRIs6A-uqym"
      },
      "id": "jrRIs6A-uqym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Time"
      ],
      "metadata": {
        "id": "sUXiJO9Funmv"
      },
      "id": "sUXiJO9Funmv"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_time(results, save_path=None):\n",
        "    \"\"\"Plot time taken per epoch and display total + average.\"\"\"\n",
        "    epoch_times = results.get(\"epoch_time\", [])\n",
        "    if not epoch_times:\n",
        "        print(\"[WARN] No epoch_time data found in results.\")\n",
        "        return\n",
        "\n",
        "    epochs = range(1, len(epoch_times) + 1)\n",
        "    avg_time = np.mean(epoch_times)\n",
        "    total_time = np.sum(epoch_times)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, epoch_times, marker='o', color='mediumseagreen', linewidth=2)\n",
        "    plt.title(\"â±ï¸ Training Time per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(epochs)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "        print(f\"âœ… Saved training time plot to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    print(f\"ðŸ“Š Total Training Time: {total_time:.2f} seconds\")\n",
        "    print(f\"âš™ï¸  Average Epoch Time: {avg_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "IjQx-Szdul0q"
      },
      "id": "IjQx-Szdul0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_time(results, save_path=\"models/checkpoints/training_time.png\")"
      ],
      "metadata": {
        "id": "9BzCiM6AWdaT"
      },
      "id": "9BzCiM6AWdaT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Predictions"
      ],
      "metadata": {
        "id": "m_lHJTU7Wgbf"
      },
      "id": "m_lHJTU7Wgbf"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, id2label, num_samples=4):\n",
        "    \"\"\"Visualize a few segmentation predictions with class colors.\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values=images)\n",
        "        logits = torch.nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        preds = torch.argmax(logits, dim=1).cpu()\n",
        "\n",
        "    # Display a few samples\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        pred_mask = preds[i].numpy()\n",
        "        true_mask = labels[i].cpu().numpy()\n",
        "\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        ax[0].imshow(img)\n",
        "        ax[0].set_title(\"Original Image\")\n",
        "\n",
        "        ax[1].imshow(true_mask, cmap=\"tab20\")\n",
        "        ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "        ax[2].imshow(pred_mask, cmap=\"tab20\")\n",
        "        ax[2].set_title(\"Predicted Mask\")\n",
        "\n",
        "        for a in ax: a.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "wovyxqJ5VKBM"
      },
      "id": "wovyxqJ5VKBM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_predictions(model, validation_loader, builder.device, builder.id2label, num_samples=4)"
      ],
      "metadata": {
        "id": "UvU2dkYNWj9-"
      },
      "id": "UvU2dkYNWj9-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}