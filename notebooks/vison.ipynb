{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "076e0c79",
      "metadata": {
        "id": "076e0c79"
      },
      "source": [
        "# Food Vision With SegFormer and Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38fc581f",
      "metadata": {
        "id": "38fc581f"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "98cf102d",
      "metadata": {
        "id": "98cf102d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Tuple, Optional, Dict, List\n",
        "from enum import Enum\n",
        "import json\n",
        "# Essentials\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "# Hugging Face\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db7ae64",
      "metadata": {
        "id": "5db7ae64"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "24555e94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24555e94",
        "outputId": "fa025208-0aea-410b-e28d-f100daa76794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ torch version: 2.8.0+cu126\n",
            "‚úÖ torchvision version: 0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from packaging import version\n",
        "\n",
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "required_torch = \"1.12.0\"\n",
        "required_torchvision = \"0.13.0\"\n",
        "\n",
        "if version.parse(torch.__version__) < version.parse(required_torch) or \\\n",
        "   version.parse(torchvision.__version__) < version.parse(required_torchvision):\n",
        "    print(\"[INFO] torch/torchvision versions not as required, installing latest versions.\")\n",
        "    # You can change cu121 to cu118 or cpu as needed\n",
        "    import os\n",
        "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "    # Reload torch and torchvision to reflect new versions\n",
        "    import importlib\n",
        "    importlib.reload(torch)\n",
        "    importlib.reload(torchvision)\n",
        "\n",
        "print(f\"‚úÖ torch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ebc846",
      "metadata": {
        "id": "00ebc846"
      },
      "source": [
        "## Setup Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive\n",
        "\n",
        "Uncomment the lines to mount google drive if using **google colab**."
      ],
      "metadata": {
        "id": "Coj0GjGfRVPg"
      },
      "id": "Coj0GjGfRVPg"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGk5o_qbRS-I",
        "outputId": "d04f6818-09a8-4c1e-f5f8-b188451698b6"
      },
      "id": "xGk5o_qbRS-I",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1c4f17",
      "metadata": {
        "id": "1b1c4f17"
      },
      "source": [
        "## Setup Device-Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f2e755d",
      "metadata": {
        "id": "9f2e755d"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9262e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9262e45",
        "outputId": "6091fb1d-13df-4508-f62f-603d619d8b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using Device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(f\"‚úÖ Using Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ef162f",
      "metadata": {
        "id": "61ef162f"
      },
      "source": [
        "## Setup Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0ea96d20",
      "metadata": {
        "id": "0ea96d20"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed: int = 42):\n",
        "    \"\"\"Sets seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8e9d5be7",
      "metadata": {
        "id": "8e9d5be7"
      },
      "outputs": [],
      "source": [
        "set_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a4ce2f",
      "metadata": {
        "id": "90a4ce2f"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc112c0e",
      "metadata": {
        "id": "dc112c0e"
      },
      "source": [
        "## Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "00f8a05a",
      "metadata": {
        "id": "00f8a05a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Optional\n",
        "from transformers import BaseImageProcessor\n",
        "from torchvision import transforms\n",
        "\n",
        "class FoodSeg103Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    A robust PyTorch Dataset wrapper for the Hugging Face FoodSeg103 dataset.\n",
        "\n",
        "    Each sample includes:\n",
        "        - image (PIL Image)\n",
        "        - label (segmentation mask as PIL Image)\n",
        "        - classes_on_image (list of class IDs present)\n",
        "        - id (int)\n",
        "\n",
        "    Supports:\n",
        "        - Hugging Face AutoImageProcessor (e.g., SegFormer processor)\n",
        "        - Optional torchvision transforms (fallback)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hf_dataset,\n",
        "        processor: Optional[BaseImageProcessor] = None,\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hf_dataset: Hugging Face dataset split (e.g., from `datasets.load_dataset`).\n",
        "            processor: Optional Hugging Face processor (handles resizing, normalization, etc.).\n",
        "            transform: Optional torchvision-style transform (used when no processor is provided).\n",
        "        \"\"\"\n",
        "        self.dataset = hf_dataset\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "        # Safety check: warn if neither processor nor transform is provided\n",
        "        if self.processor is None and self.transform is None:\n",
        "            print(\"‚ö†Ô∏è Warning: No processor or transform provided. \"\n",
        "                  \"Images will remain as PIL and may cause DataLoader errors.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetch and preprocess one sample from the dataset.\n",
        "        Returns:\n",
        "            (pixel_values, labels): both torch.Tensors\n",
        "        \"\"\"\n",
        "        item = self.dataset[idx]\n",
        "        image = item[\"image\"]   # PIL Image\n",
        "        label = item[\"label\"]   # PIL Image mask\n",
        "\n",
        "        # --- Option 1: Use Hugging Face processor (preferred for SegFormer) ---\n",
        "        if self.processor is not None:\n",
        "            encoded = self.processor(image, segmentation_maps=label, return_tensors=\"pt\")\n",
        "            pixel_values = encoded[\"pixel_values\"].squeeze(0)\n",
        "            labels = encoded[\"labels\"].squeeze(0).long()\n",
        "            return pixel_values, labels\n",
        "\n",
        "        # --- Option 2: Use torchvision transforms manually ---\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Fallback: convert PIL ‚Üí Tensor to avoid DataLoader crashes\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        label = np.array(label, dtype=np.int64)\n",
        "        label = torch.as_tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a clean summary of the dataset configuration.\"\"\"\n",
        "        transform_str = str(self.transform) if self.transform else \"None\"\n",
        "        processor_str = self.processor.__class__.__name__ if self.processor else \"None\"\n",
        "        split_name = getattr(self.dataset, \"split\", \"unknown\")\n",
        "        return (\n",
        "            f\"Dataset: FoodSeg103\\n\"\n",
        "            f\"    Number of datapoints: {len(self)}\\n\"\n",
        "            f\"    Split: {split_name}\\n\"\n",
        "            f\"    Processor: {processor_str}\\n\"\n",
        "            f\"    Transform: {transform_str}\\n\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Mappings"
      ],
      "metadata": {
        "id": "_ruQ_i7wOTeA"
      },
      "id": "_ruQ_i7wOTeA"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_class_mappings(json_path: str) -> Tuple[Optional[Dict[int, str]], Optional[Dict[int, str]]]:\n",
        "    \"\"\"\n",
        "    Load id2label and label2id mappings from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        json_path (str): Path to the JSON file like:\n",
        "            {\n",
        "              \"0\": \"background\",\n",
        "              \"1\": \"candy\",\n",
        "              \"2\": \"egg tart\",\n",
        "              ...\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        tuple: (id2label: dict[int, str], label2id: dict[str, int])\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert string keys to integers for id2label\n",
        "    id2label = {int(k): v for k, v in data.items()}\n",
        "    label2id = {v: int(k) for k, v in id2label.items()}\n",
        "\n",
        "    return id2label, label2id"
      ],
      "metadata": {
        "id": "YwBs7Ub0Uo3q"
      },
      "id": "YwBs7Ub0Uo3q",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label, label2id = load_class_mappings(\"/content/drive/MyDrive/datasets/foodSeg103/classnames.json\")\n",
        "\n",
        "print(\"First 5 id2label:\")\n",
        "print(dict(list(id2label.items())[:5]))\n",
        "\n",
        "print(\"\\nFirst 5 label2id:\")\n",
        "print(dict(list(label2id.items())[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3hKVRLikoqX",
        "outputId": "3ea6e812-a5ed-4ba5-d50c-cae9c9667735"
      },
      "id": "Q3hKVRLikoqX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 id2label:\n",
            "{0: 'background', 1: 'candy', 2: 'egg tart', 3: 'french fries', 4: 'chocolate'}\n",
            "\n",
            "First 5 label2id:\n",
            "{'background': 0, 'candy': 1, 'egg tart': 2, 'french fries': 3, 'chocolate': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581c29de",
      "metadata": {
        "id": "581c29de"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ae275c84",
      "metadata": {
        "id": "ae275c84"
      },
      "outputs": [],
      "source": [
        "class Split(Enum):\n",
        "    TRAIN = \"train\"\n",
        "    VALIDATION = \"validation\"\n",
        "    TEST = \"test\"\n",
        "    ALL = \"all\"\n",
        "\n",
        "def create_dataloaders_from_hf(\n",
        "    dataset_name: str,\n",
        "    split: Split = Split.ALL,\n",
        "    processor: Optional[BaseImageProcessor] = None,\n",
        "    batch_size: int = 4,\n",
        "    num_workers: int = 2,\n",
        "    device: str = None\n",
        ") -> Tuple[Optional[torch.utils.data.DataLoader], Optional[torch.utils.data.DataLoader], Optional[torch.utils.data.DataLoader]]:\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders from a Hugging Face dataset like FoodSeg103.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Hugging Face dataset name.\n",
        "        transform: torchvision transforms to perform on data.\n",
        "        split (Split): Which split(s) to load (TRAIN, VALIDATION, TEST, or ALL).\n",
        "        batch_size (int): Batch size for DataLoaders.\n",
        "        num_workers (int): Number of DataLoader workers.\n",
        "    Returns:\n",
        "        (train_loader, val_loader, test_loader): Tuple of DataLoaders.\n",
        "        Unused splits will be returned as None.\n",
        "    \"\"\"\n",
        "\n",
        "    def build_loader(split_name: str, is_train: bool = False, device:str = None) -> torch.utils.data.DataLoader:\n",
        "        ds = load_dataset(dataset_name, split=split_name)\n",
        "        dataset = FoodSeg103Dataset(ds, processor=processor)\n",
        "        device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pin_mem = True if device == \"cuda\" else False\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=is_train,\n",
        "            drop_last=is_train,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_mem,\n",
        "        )\n",
        "\n",
        "    # --- Load splits ---\n",
        "    train_loader = val_loader = test_loader = None\n",
        "    if split in (Split.TRAIN, Split.ALL):\n",
        "        train_loader = build_loader(\"train\", is_train=True)\n",
        "\n",
        "    if split in (Split.VALIDATION, Split.ALL):\n",
        "        val_loader = build_loader(\"validation\")\n",
        "\n",
        "    if split in (Split.TEST, Split.ALL):\n",
        "        try:\n",
        "            test_loader = build_loader(\"test\")\n",
        "        except ValueError:\n",
        "            print(f\"‚ö†Ô∏è No 'test' split found in dataset: {dataset_name}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30aa13ae",
      "metadata": {
        "id": "30aa13ae"
      },
      "source": [
        "# Model Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7362b70f",
      "metadata": {
        "id": "7362b70f"
      },
      "source": [
        "## SegFormer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5107111e",
      "metadata": {
        "id": "5107111e"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation, SegformerConfig, SegformerImageProcessor, BaseImageProcessor\n",
        "\n",
        "class Segformer:\n",
        "    \"\"\"\n",
        "    A flexible builder for creating and configuring SegFormer models\n",
        "    from the Hugging Face Transformers library.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_classes: int = 104,\n",
        "        ignore_mismatched_sizes: bool = True,\n",
        "        dropout: float = 0.1,\n",
        "        device: str = \"cuda\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the SegformerBuilder.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Pretrained SegFormer model name or path.\n",
        "            num_classes (int): Number of segmentation classes.\n",
        "            ignore_mismatched_sizes (bool): Allow different output head sizes.\n",
        "            use_pretrained (bool): Whether to load pretrained weights.\n",
        "            device (str): Device to load model on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_mismatched_sizes = ignore_mismatched_sizes\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load model config Parameters\n",
        "        id2label, label2id = load_class_mappings(\"/content/drive/MyDrive/datasets/foodSeg103/classnames.json\")\n",
        "        self.config = SegformerConfig.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_classes,\n",
        "            hidden_dropout_prob=dropout,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        )\n",
        "\n",
        "        # Load model weights\n",
        "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "            model_name,\n",
        "            config=self.config,\n",
        "            ignore_mismatched_sizes=ignore_mismatched_sizes\n",
        "        )\n",
        "\n",
        "        # Load image processor (for transforms)\n",
        "        self.processor = SegformerImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "        # Set model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_model(self) -> torch.nn.Module:\n",
        "        \"\"\"Return the PyTorch model.\"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def get_processor(self):\n",
        "        \"\"\"Return the Hugging Face image processor for transforms.\"\"\"\n",
        "        return self.processor\n",
        "\n",
        "    def freeze_encoder(self, freeze=True):\n",
        "        \"\"\"Freeze or unfreeze the encoder layers.\"\"\"\n",
        "        for param in self.model.segformer.encoder.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "        print(f\"Encoder frozen: {freeze}\")\n",
        "\n",
        "    def freeze_decoder(self, freeze=True):\n",
        "        \"\"\"Freeze or unfreeze the decoder layers.\"\"\"\n",
        "        for param in self.model.decode_head.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "        print(f\"Decoder frozen: {freeze}\")\n",
        "\n",
        "    def unfreeze_all(self):\n",
        "        \"\"\"Unfreeze all parameters.\"\"\"\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"All model parameters are trainable.\")\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Print basic info about the model.\"\"\"\n",
        "        print(\"\\nüß© SegFormer Model Summary\\n\")\n",
        "        print(f\"Model: {self.model_name}\")\n",
        "        print(f\"Classes: {self.num_classes}\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        print(f\"Trainable: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
        "        # Add shape info\n",
        "        size = self.processor.size.get(\"height\", 512)\n",
        "        dummy = torch.randn(1, 3, size, size).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.model(pixel_values=dummy)\n",
        "        print(f\"Output keys: {list(out.keys())}\")\n",
        "        print(f\"Logits shape: {out.logits.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45c24ca",
      "metadata": {
        "id": "b45c24ca"
      },
      "source": [
        "## Training Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization Helpers"
      ],
      "metadata": {
        "id": "a_zEzsemL651"
      },
      "id": "a_zEzsemL651"
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_image_with_mask(image, mask=None, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Visualize an image, its segmentation mask, and overlay.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray or PIL.Image.Image): RGB image.\n",
        "        mask (np.ndarray or PIL.Image.Image): Segmentation mask (H x W).\n",
        "        alpha (float): Transparency for overlay blending.\n",
        "    \"\"\"\n",
        "    # Convert PIL ‚Üí NumPy if needed\n",
        "    img = np.array(image)\n",
        "    if img.dtype != np.uint8:\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    mask_np = None\n",
        "    if mask is not None:\n",
        "        mask_np = np.array(mask, dtype=np.int64)\n",
        "\n",
        "        # --- Resize mask to match image if needed ---\n",
        "        if mask_np.shape[:2] != (h, w):\n",
        "            mask_np = cv2.resize(\n",
        "                mask_np, (w, h), interpolation=cv2.INTER_NEAREST\n",
        "            )\n",
        "\n",
        "        # --- Random color map for classes ---\n",
        "        unique_classes = np.unique(mask_np)\n",
        "        color_map = {cls: np.random.randint(0, 255, size=3) for cls in unique_classes if cls != 0}\n",
        "\n",
        "        color_mask = np.zeros_like(img)\n",
        "        for cls_id, color in color_map.items():\n",
        "            color_mask[mask_np == cls_id] = color\n",
        "\n",
        "        # Blend image + color mask\n",
        "        blended = cv2.addWeighted(img, 1 - alpha, color_mask, alpha, 0)\n",
        "    else:\n",
        "        blended = img\n",
        "\n",
        "    # --- Plot all ---\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if mask_np is not None:\n",
        "        plt.imshow(mask_np, cmap=\"tab20\")\n",
        "        plt.title(\"Segmentation Mask (Class IDs)\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No mask\", ha=\"center\", va=\"center\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(blended)\n",
        "    plt.title(\"Overlay (Image + Mask)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Metadata ---\n",
        "    if mask_np is not None:\n",
        "        coverage = np.count_nonzero(mask_np) / mask_np.size * 100\n",
        "        print(f\"‚úÖ Mask Coverage: {coverage:.2f}% of image\")\n",
        "        print(f\"üü¢ Unique classes: {np.unique(mask_np).tolist()}\")\n",
        "        print(f\"üìè Image size: {img.shape}\")\n",
        "        print(f\"üé≠ Mask size: {mask_np.shape}\")"
      ],
      "metadata": {
        "id": "GoK_6_1D6WDz"
      },
      "id": "GoK_6_1D6WDz",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unnormalize_image(tensor: torch.Tensor, mean: Tuple[float, float, float]=(0.485, 0.456, 0.406), std: Tuple[float, float, float]=(0.229, 0.224, 0.225)):\n",
        "    \"\"\"\n",
        "    Reverse ImageNet normalization to make images display correctly.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Normalized image tensor of shape (C, H, W)\n",
        "        mean (tuple): Mean used in normalization\n",
        "        std (tuple): Std used in normalization\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Unnormalized image tensor, values clamped to [0, 1]\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone().cpu()\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return torch.clamp(tensor, 0, 1)\n",
        "\n",
        "\n",
        "def plot_segmentation(images: torch.Tensor, masks: torch.Tensor, preds: torch.Tensor, n:int=4, save_path: str=None, unnormalize:bool=True):\n",
        "    \"\"\"\n",
        "    Visualize input images, ground truth masks, and model predictions.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images (B, C, H, W)\n",
        "        masks (torch.Tensor): Ground truth segmentation masks (B, H, W)\n",
        "        preds (torch.Tensor): Predicted segmentation masks (B, H, W)\n",
        "        n (int): Number of samples to visualize\n",
        "        save_path (str): Optional file path to save visualization\n",
        "        unnormalize (bool): Whether to unnormalize images before plotting\n",
        "    \"\"\"\n",
        "    # Ensure tensors are on CPU and detached\n",
        "    images, masks, preds = images.cpu(), masks.cpu(), preds.cpu()\n",
        "\n",
        "    plt.figure(figsize=(12, n * 3))\n",
        "\n",
        "    for i in range(min(n, images.size(0))):\n",
        "        # --- Reverse normalization for accurate visualization ---\n",
        "        if unnormalize:\n",
        "            img = unnormalize_image(images[i])\n",
        "        else:\n",
        "            img = torch.clamp(images[i], 0, 1)\n",
        "\n",
        "        img = img.permute(1, 2, 0).numpy()  # Convert from CHW ‚Üí HWC\n",
        "\n",
        "        # --- Original Image ---\n",
        "        plt.subplot(n, 3, i * 3 + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # --- Ground Truth Mask ---\n",
        "        plt.subplot(n, 3, i * 3 + 2)\n",
        "        plt.imshow(masks[i], cmap=\"tab20\")\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # --- Prediction Mask ---\n",
        "        plt.subplot(n, 3, i * 3 + 3)\n",
        "        plt.imshow(preds[i], cmap=\"tab20\")\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- Optionally save figure ---\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "        print(f\"üñºÔ∏è Saved segmentation visualization to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_curves(results, save_path=None):\n",
        "    \"\"\"Visualize training metrics (loss, acc, mIoU) over epochs.\"\"\"\n",
        "    train_loss, test_loss = results[\"train_loss\"], results[\"test_loss\"]\n",
        "    train_acc, test_acc = results[\"train_acc\"], results[\"test_acc\"]\n",
        "    train_iou, test_iou = results[\"train_iou\"], results[\"test_iou\"]\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 5))\n",
        "\n",
        "    # ---- Loss Plot ----\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, train_loss, \"b-\", label=\"Train Loss\")\n",
        "    plt.plot(epochs, test_loss, \"r-\", label=\"Test Loss\")\n",
        "    plt.title(\"Loss per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "\n",
        "    # ---- Accuracy Plot ----\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, train_acc, \"b-\", label=\"Train Acc\")\n",
        "    plt.plot(epochs, test_acc, \"r-\", label=\"Test Acc\")\n",
        "    plt.title(\"Accuracy per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "\n",
        "    # ---- mIoU Plot ----\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, train_iou, \"b-\", label=\"Train mIoU\")\n",
        "    plt.plot(epochs, test_iou, \"r-\", label=\"Test mIoU\")\n",
        "    plt.title(\"Mean IoU per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "        print(f\"‚úÖ Saved training curves to {save_path}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jfdmE5kML5xY"
      },
      "id": "jfdmE5kML5xY",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_random_sample(dataset: torch.utils.data.Dataset, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Displays a random image and its mask from a PyTorch dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): A PyTorch dataset that returns (image, mask) pairs.\n",
        "    \"\"\"\n",
        "    # Pick random index\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    image, mask = dataset[idx]\n",
        "\n",
        "    # If tensors ‚Üí convert to NumPy (for plotting)\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image_np = image.permute(1, 2, 0).numpy()  # CHW ‚Üí HWC\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "    else:\n",
        "        image_np = np.array(image)\n",
        "\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask_np = mask.numpy()\n",
        "    else:\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "    # --- Print metadata ---\n",
        "    print(f\"üñºÔ∏è Sample Index: {idx}\")\n",
        "    # --- Visualize using helper ---\n",
        "    visualize_image_with_mask(image_np, mask_np)"
      ],
      "metadata": {
        "id": "VeH8PRas6hus"
      },
      "id": "VeH8PRas6hus",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Helpers"
      ],
      "metadata": {
        "id": "UDsOUbADMHgR"
      },
      "id": "UDsOUbADMHgR"
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_accuracy(preds, labels):\n",
        "    \"\"\"Compute per-pixel accuracy.\"\"\"\n",
        "    correct = (preds == labels).sum().item()\n",
        "    total = labels.numel()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def intersection_over_union(preds, labels, num_classes: int, ignore_index:int=255):\n",
        "    \"\"\"Compute mean Intersection-over-Union (mIoU).\"\"\"\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "    ious = []\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        if cls == ignore_index:\n",
        "            continue\n",
        "        pred_inds = preds == cls\n",
        "        label_inds = labels == cls\n",
        "        intersection = np.logical_and(pred_inds, label_inds).sum()\n",
        "        union = np.logical_or(pred_inds, label_inds).sum()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "\n",
        "    return np.mean(ious) if ious else 0.0"
      ],
      "metadata": {
        "id": "3xXwxXteMKhA"
      },
      "id": "3xXwxXteMKhA",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpoint Utilities"
      ],
      "metadata": {
        "id": "CGszaJPHMQ6L"
      },
      "id": "CGszaJPHMQ6L"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(\n",
        "    model: torch.nn.Module,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epoch: int,\n",
        "    best_miou,\n",
        "    path: Optional[str] = \"ckpts\",\n",
        "    filename: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Save model, optimizer, scheduler, and mIoU to a checkpoint file.\n",
        "    The file will be named as segformer_finetuned_{epoch}.ckpt under\n",
        "    ckpt directory by default\n",
        "    \"\"\"\n",
        "    # Create Directory Path\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    # Filename\n",
        "    filename = f\"segformer_finetuned_{epoch}.ckpt\"\n",
        "    checkpoint_path = os.path.join(path, filename)\n",
        "    # Save Model\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict() if optimizer else None,\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
        "        \"best_miou\": best_miou,\n",
        "    }, checkpoint_path)\n",
        "    # Log\n",
        "    print(f\"‚úÖ Checkpoint saved at: {checkpoint_path} with mIoU: {best_miou}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    model: torch.nn.Module,\n",
        "    path: str,\n",
        "    optimizer=None,\n",
        "    scheduler=None,\n",
        "    device: Optional[str] = \"cuda\"):\n",
        "    \"\"\"Load model and optimizer states from a checkpoint.\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    if scheduler and checkpoint[\"scheduler_state_dict\"]:\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "    # Log\n",
        "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint['epoch'] + 1}, \"\n",
        "          f\"best mIoU={checkpoint['results']:.4f}\")\n",
        "    return checkpoint"
      ],
      "metadata": {
        "id": "4BIoNMNOMLaS"
      },
      "id": "4BIoNMNOMLaS",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainning Loops"
      ],
      "metadata": {
        "id": "pZCIMM3d4A8W"
      },
      "id": "pZCIMM3d4A8W"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6ce5bf0f",
      "metadata": {
        "id": "6ce5bf0f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# ============================================================\n",
        "# üîπ Training / Evaluation Steps (with AMP)\n",
        "# ============================================================\n",
        "\n",
        "def train_step(model, dataloader: torch.utils.data.DataLoader, scaler, optimizer, loss_fn, num_classes:int, device:str):\n",
        "    \"\"\"Run one training epoch.\"\"\"\n",
        "    model.train()\n",
        "    train_loss, train_acc, train_iou = 0, 0, 0\n",
        "\n",
        "    # Automatically choose AMP device (cuda/cpu)\n",
        "    autocast_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    for images, labels in tqdm(dataloader, leave=False, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ---- Forward + Backward with AMP ----\n",
        "        with autocast(device_type=autocast_device, enabled=torch.cuda.is_available()):\n",
        "            outputs = model(pixel_values=images)\n",
        "            logits = torch.nn.functional.interpolate(\n",
        "                outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        # ---- Gradient Scaling ----\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # ---- Metrics ----\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        train_acc += pixel_accuracy(preds, labels)\n",
        "        train_iou += intersection_over_union(preds, labels, num_classes)\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return train_loss / n, train_acc / n, train_iou / n\n",
        "\n",
        "\n",
        "def eval_step(model, dataloader: torch.utils.data.DataLoader, loss_fn, num_classes:int, device:str):\n",
        "    \"\"\"Evaluate model for one epoch.\"\"\"\n",
        "    model.eval()\n",
        "    test_loss, test_acc, test_iou = 0, 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in tqdm(dataloader, leave=False, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(pixel_values=images)\n",
        "            logits = torch.nn.functional.interpolate(\n",
        "                outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "            loss = loss_fn(logits, labels)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # ---- Metrics ----\n",
        "            test_loss += loss.item()\n",
        "            test_acc += pixel_accuracy(preds, labels)\n",
        "            test_iou += intersection_over_union(preds, labels, num_classes)\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return test_loss / n, test_acc / n, test_iou / n\n",
        "\n",
        "# ============================================================\n",
        "# üîπ Full Training Loop (with Auto-Resume + Visualization)\n",
        "# ============================================================\n",
        "\n",
        "def train(model, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer, loss_fn, epochs: int, device: str,\n",
        "          num_classes:int, scheduler=None, save_dir:str=\"checkpoints\", min_best_miou: float =0.0, vis_every:int=5, auto_save: bool = True):\n",
        "    \"\"\"\n",
        "    Full training loop for semantic segmentation with:\n",
        "    - AMP training\n",
        "    - Auto resume\n",
        "    - Periodic visualization and checkpoints\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    model.to(device)\n",
        "    best_miou = 0.0\n",
        "\n",
        "    # ---- Auto Resume ----\n",
        "    best_ckpt_path = os.path.join(save_dir, \"best_segformer.ckpt\")\n",
        "    if os.path.exists(best_ckpt_path):\n",
        "        ckpt = load_checkpoint(model, optimizer, scheduler, path=best_ckpt_path)\n",
        "        best_miou = ckpt[\"best_miou\"]\n",
        "\n",
        "    # ---- Training Logs ----\n",
        "    results = {k: [] for k in [\"train_loss\", \"train_acc\", \"train_iou\",\n",
        "                               \"test_loss\", \"test_acc\", \"test_iou\", \"epoch_time\"]}\n",
        "\n",
        "    # ---- Training Loop ----\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.perf_counter()\n",
        "        print(f\"\\nüå± Epoch [{epoch+1}/{epochs}]\")\n",
        "\n",
        "        # ---- Train and Evaluate ----\n",
        "        train_loss, train_acc, train_iou = train_step(model, train_dataloader,scaler, optimizer, loss_fn, num_classes, device)\n",
        "        test_loss, test_acc, test_iou = eval_step(model, test_dataloader, loss_fn, num_classes, device)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # ---- Logging ----\n",
        "        epoch_time = time.perf_counter() - start_time\n",
        "        results[\"epoch_time\"].append(epoch_time)\n",
        "        tqdm.write(\n",
        "            f\"üïí {epoch_time:.2f}s\\n\"\n",
        "            f\"üìà Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | mIoU: {train_iou:.4f}\\n\"\n",
        "            f\"üìà Validation Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | mIoU: {test_iou:.4f}\\n\"\n",
        "        )\n",
        "\n",
        "        # ---- Save Best Model ----\n",
        "        if test_iou > best_miou and test_iou >= min_best_miou:\n",
        "            best_miou = test_iou\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, best_miou, path=save_dir)\n",
        "\n",
        "        # ---- Periodic Tasks ----\n",
        "        if (epoch + 1) % vis_every == 0:\n",
        "            # Visualize Predictions after vis_every interval\n",
        "            model.eval()\n",
        "            images, labels = next(iter(test_dataloader))\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                outputs = model(pixel_values=images)\n",
        "                logits = torch.nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            plot_segmentation(images, labels, preds, n=min(4, images.size(0)),\n",
        "                              save_path=os.path.join(save_dir, f\"vis_epoch_{epoch+1}.png\"))\n",
        "            plot_training_curves(results, save_path=os.path.join(save_dir, \"training_curves.png\"))\n",
        "            # Save Checkpoint after every vis_every\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, best_miou, path=save_dir, filename=f\"segformer_epoch_{epoch+1}.ckpt\")\n",
        "\n",
        "        # ---- Record Metrics ----\n",
        "        for k, v in zip([\"train_loss\", \"train_acc\", \"train_iou\", \"test_loss\", \"test_acc\", \"test_iou\"],\n",
        "                        [train_loss, train_acc, train_iou, test_loss, test_acc, test_iou]):\n",
        "            results[k].append(v)\n",
        "\n",
        "    print(\"\\nüéØ Training complete!\")\n",
        "    print(f\"üèÜ Best Validation mIoU: {best_miou:.4f}\\n\")\n",
        "    avg_time = np.mean(results[\"epoch_time\"])\n",
        "    print(f\"‚è±Ô∏è Average Epoch Time: {avg_time:.2f} sec\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d124c05a",
      "metadata": {
        "id": "d124c05a"
      },
      "source": [
        "## Fine Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b909cb3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b909cb3f",
        "outputId": "830634ad-b1c1-4b2c-ec97-643a5b4335a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([104]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([104, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n",
            "  image_processor = cls(**image_processor_dict)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No 'test' split found in dataset: EduardoPacheco/FoodSeg103\n",
            "\n",
            "üß© SegFormer Model Summary\n",
            "\n",
            "Model: nvidia/segformer-b0-finetuned-ade-512-512\n",
            "Classes: 104\n",
            "Device: cpu\n",
            "Parameters: 3,740,872\n",
            "Trainable: 3,740,872\n",
            "Output keys: ['logits']\n",
            "Logits shape: torch.Size([1, 104, 128, 128])\n",
            "None\n",
            "\n",
            "üå± Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 6/622 [03:32<5:42:27, 33.36s/it]"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "T_MAX = 20\n",
        "IGNORE_INDEX = 255\n",
        "CHECKPOINT_DIR = \"drive/MyDrive/checkpoints\"\n",
        "NUM_WORKERS = os.cpu_count() or 2 # Use 2 Workers as default\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 104\n",
        "DROP_RATE = 0.1\n",
        "\n",
        "# Initialize model and builder\n",
        "builder = Segformer(num_classes=NUM_CLASSES, device=DEVICE, dropout=DROP_RATE)\n",
        "model = builder.get_model()\n",
        "# Get Image Processor from builder\n",
        "processor = builder.get_processor()\n",
        "\n",
        "# Initialize Dataloaders\n",
        "train_loader, validation_loader, _ = create_dataloaders_from_hf(dataset_name=\"EduardoPacheco/FoodSeg103\", batch_size=BATCH_SIZE, processor=processor)\n",
        "\n",
        "# Loss (ignore_index=255 if present in dataset)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)\n",
        "\n",
        "# Model Summary\n",
        "print(builder.summary())\n",
        "\n",
        "# Fine-tune\n",
        "if __name__ == \"__main__\":\n",
        "    results = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=validation_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=EPOCHS,\n",
        "    device=DEVICE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    scheduler=scheduler,\n",
        "    save_dir=CHECKPOINT_DIR,\n",
        "    vis_every=8,  # visualize predictions every 8 epochs\n",
        ")\n",
        "\n",
        "# Later (for inference or resuming)\n",
        "# load_checkpoint(model, path=\"./ckspts/segformer_best.ckpt\", device=builder.device)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854c2b4b",
      "metadata": {
        "id": "854c2b4b"
      },
      "source": [
        "## Save Model Checkpoint Manually\n",
        "\n",
        "The model can be saved via `model.save_pretrained(\"./segformer_finetuned\")` or `builder.get_processor().save_pretrained(\"./segformer_finetuned\")`. any of the method works.\n",
        "\n",
        "You can use this code for loading the model in inference.\n",
        "```python\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "# Load model wither via `SegformerForSemanticSegmentation`\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"./ckpts/segformer_finetuned\")\n",
        "# the mode can also be loaded via AutoImageProcessor\n",
        "processor = SegformerImageProcessor.from_pretrained(\"./ckpts/segformer_finetuned\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e71f56",
      "metadata": {
        "id": "11e71f56"
      },
      "outputs": [],
      "source": [
        "model_path=\"./segformer_finetuned\"\n",
        "# model.save_pretrained(\"./segformer_finetuned\")\n",
        "builder.get_processor().save_pretrained(model_path)\n",
        "print(f\"üé´Saved Pretained Processor to: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "fonwwG0MuPxw"
      },
      "id": "fonwwG0MuPxw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Loss Curves"
      ],
      "metadata": {
        "id": "QpIANSequXEw"
      },
      "id": "QpIANSequXEw"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(results, save_path=\"models/checkpoints/training_plots.png\")"
      ],
      "metadata": {
        "id": "jrRIs6A-uqym"
      },
      "id": "jrRIs6A-uqym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Time"
      ],
      "metadata": {
        "id": "sUXiJO9Funmv"
      },
      "id": "sUXiJO9Funmv"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_time(results, save_path=None):\n",
        "    \"\"\"Plot time taken per epoch and display total + average.\"\"\"\n",
        "    epoch_times = results.get(\"epoch_time\", [])\n",
        "    if not epoch_times:\n",
        "        print(\"[WARN] No epoch_time data found in results.\")\n",
        "        return\n",
        "\n",
        "    epochs = range(1, len(epoch_times) + 1)\n",
        "    avg_time = np.mean(epoch_times)\n",
        "    total_time = np.sum(epoch_times)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, epoch_times, marker='o', color='mediumseagreen', linewidth=2)\n",
        "    plt.title(\"‚è±Ô∏è Training Time per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(epochs)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "        print(f\"‚úÖ Saved training time plot to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    print(f\"üìä Total Training Time: {total_time:.2f} seconds\")\n",
        "    print(f\"‚öôÔ∏è  Average Epoch Time: {avg_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "IjQx-Szdul0q"
      },
      "id": "IjQx-Szdul0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_time(results, save_path=\"models/checkpoints/training_time.png\")"
      ],
      "metadata": {
        "id": "9BzCiM6AWdaT"
      },
      "id": "9BzCiM6AWdaT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Predictions"
      ],
      "metadata": {
        "id": "m_lHJTU7Wgbf"
      },
      "id": "m_lHJTU7Wgbf"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, id2label, num_samples=4):\n",
        "    \"\"\"Visualize a few segmentation predictions with class colors.\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values=images)\n",
        "        logits = torch.nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        preds = torch.argmax(logits, dim=1).cpu()\n",
        "\n",
        "    # Display a few samples\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        pred_mask = preds[i].numpy()\n",
        "        true_mask = labels[i].cpu().numpy()\n",
        "\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        ax[0].imshow(img)\n",
        "        ax[0].set_title(\"Original Image\")\n",
        "\n",
        "        ax[1].imshow(true_mask, cmap=\"tab20\")\n",
        "        ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "        ax[2].imshow(pred_mask, cmap=\"tab20\")\n",
        "        ax[2].set_title(\"Predicted Mask\")\n",
        "\n",
        "        for a in ax: a.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "wovyxqJ5VKBM"
      },
      "id": "wovyxqJ5VKBM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_predictions(model, validation_loader, builder.device, builder.id2label, num_samples=4)"
      ],
      "metadata": {
        "id": "UvU2dkYNWj9-"
      },
      "id": "UvU2dkYNWj9-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}